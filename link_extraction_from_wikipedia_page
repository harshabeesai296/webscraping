import requests
import urllib3
import json
from bs4 import BeautifulSoup
import sys
from unidecode import unidecode
import time

sb = requests.get('https://en.wikipedia.org/wiki/Nature')
html = sb.text

mysoup = BeautifulSoup(html, 'html.parser')
print(mysoup.prettify().encode("utf-8"))
print(mysoup.title.text)
#time.sleep(60)

psnat = mysoup.find('p.b')
for link in mysoup.find_all("p"):
    with open('natlinks.txt', 'w') as cv:
        cv.write(str(link).encode('utf-8'))


# One more code snippet for the same purpose

import requests
from bs4 import BeautifulSoup


resp = requests.get('https://en.wikipedia.org/wiki/Bitcoin')
reqcont = resp.content

# The HTML document generated by Requests is raw,unstructured and very difficult to navigate
with open('bitcoinreq.txt','wb') as nf:
    nf.write(reqcont)

# The HTML document prettified with BS4 is much more structured and easy to navigate
mysoup = BeautifulSoup(resp.content,'html.parser')
with open('bitcoinbs4.txt','wb') as nf:
    nf.write(mysoup.prettify().encode('utf-8'))

# We can extract all the text in the HTML page with this method
with open('btctext.xlsx','wb') as nf:
    nf.write(mysoup.text.encode('utf-8'))

# Below are few of the methods we can use on the BS object
print(mysoup.title)
print(mysoup.title.name)
print(mysoup.title.string)

print(mysoup.title.parent.name)
print(mysoup.p['class'])
print(mysoup.a['id'])

alist = mysoup.find_all('a')
lgt = len(alist)
print(lgt)

f = open('alinks.txt','w')
for i in alist:
    f.write(str(i.encode('utf-8'))+'\n')

ahrefs = []
for links in mysoup.find_all('a'):
    ahrefs.append(links.get('href'))

https = "https://www.wikipedia.org"

ahrefs = [https + str(s) for s in ahrefs]
print(ahrefs)
link = open('wikilinks.txt','w')
for i in ahrefs:
    link.write(str(i.encode('utf-8')) + '\n')
